pipelines:
  branches:
    dev:
      - step:
          name: ðŸ—ï¸ Build and Push Docker Image
          runs-on:
            - self.hosted
            - linux.shell
          script:
            # CI: Build and push the versioned image
            - export DOCKER_HOST=unix:///var/run/docker.sock
            - echo "Logging in to Docker Hub..."
            - echo "$DOCKERHUB_PASSWORD" | docker login -u "$DOCKERHUB_USERNAME" --password-stdin
            - IMAGE_NAME="${DOCKERHUB_USERNAME}/automation-project"
            - VERSION="prod-0.1.${BITBUCKET_BUILD_NUMBER}"
            - VERSIONED_IMAGE="${IMAGE_NAME}:${VERSION}"
            - echo "Building Docker image..."
            - docker build -t "${IMAGE_NAME}:latest" .
            - docker tag "${IMAGE_NAME}:latest" "${VERSIONED_IMAGE}"
            - echo "Pushing images..."
            - docker push "${IMAGE_NAME}:latest"
            - docker push "${VERSIONED_IMAGE}"
            - echo "Image pushed successfully!"

      - step:
          name: âš™ï¸ Provision & Setup Target Nodes
          runs-on:
            - self.hosted
            - linux.shell
          script:
            - export TARGET_HOST_MASTER="192.168.1.34"
            - export TARGET_HOST_WORKER="192.168.1.13"
            - export DEPLOY_USER_MASTER="ikarl"
            - export DEPLOY_USER_WORKER="deploy"
            
            # 1. Install prerequisites (Ansible and SSH client)
            - sudo apt-get update && sudo apt-get install -y ansible openssh-client
            
            # 2. Configure SSH Key (Required for all connections)
            - mkdir -p ~/.ssh
            # FIX: Clean key of Windows characters
            - echo "$SSH_PRIVATE_KEY" | sed 's/\r$//' > ~/.ssh/id_rsa
            - chmod 600 ~/.ssh/id_rsa
            
            # 3. START/USE SSH AGENT AND ADD KEY (Guaranteed to work with a clean key)
            - eval "$(ssh-agent -s)" 
            - ssh-add ~/.ssh/id_rsa
            
            # 4. Add Host Fingerprints (Accepts keys for both targets)
            - ssh-keyscan -H $TARGET_HOST_MASTER >> ~/.ssh/known_hosts
            - ssh-keyscan -H $TARGET_HOST_WORKER >> ~/.ssh/known_hosts

            # 5. EXECUTE ANSIBLE PLAYBOOK (Cluster Setup)
            # This requires cluster-setup.yml to be present and to use the correct users.
            - echo "Executing Ansible playbook for cluster configuration..."
            - ansible-playbook -i inventory.ini cluster-setup.yml --user "$DEPLOY_USER_MASTER"
            
            - echo "âœ… Cluster configuration complete."

      - step:
          name: ðŸš€ Deploy App (Kubernetes & Final Ansible Task)
          runs-on:
            - self.hosted
            - linux.shell
          deployment: Production
          script:
            # --- KUBERNETES DEPLOYMENT (App Orchestration) ---
            - export CLONE_DIR="${BITBUCKET_CLONE_DIR}" 
            - export K8S_MANIFEST_PATH="k8s/deployment.yaml"
            - cd "${CLONE_DIR}"
            
            # 1. Patch and Apply
            - DEPLOY_IMAGE="${DOCKERHUB_USERNAME}/automation-project:latest"
            - echo "Patching manifest with new image tag"
            - sed -i "s|<IMAGE_PLACEHOLDER>|${DEPLOY_IMAGE}|g" "${K8S_MANIFEST_PATH}"
            - kubectl apply -f "${K8S_MANIFEST_PATH}"
            
            # 2. Force Restart and Verify
            - kubectl rollout restart deployment/automation-project-deployment
            - kubectl rollout status deployment/automation-project-deployment
            
            # --- FINAL ANSIBLE TASK (e.g., Post-deployment cleanup on Worker Node) ---
            - echo "Executing final Ansible deployment task..."
            # Note: This runs deploy.yml against the kube_workers group implicitly using the agent.
            - ansible-playbook -i inventory.ini deploy.yml --user "$DEPLOY_USER_WORKER"

            # 3. Clean up agent and key file
            - ssh-add -D 
            - rm ~/.ssh/id_rsa
            - echo "âœ… Hybrid Deployment successful!"